{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from text_features import bag_of_words, tf_idf, spacy_appoach\n",
    "from img_features import resnet50_features\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\")\n",
    "imgs_dir = data_dir / \"imgs\"\n",
    "embeds_dir = Path(\"embeddings\")\n",
    "\n",
    "movie_info = pd.read_csv(data_dir / \"movie_info.csv\")\n",
    "movie_info[\"plot\"].fillna(\"No description\", inplace=True)\n",
    "movie_info[\"genres\"] = movie_info[\"genres\"].map(lambda x: json.loads(x.replace(\"\\'\", \"\\\"\")))\n",
    "\n",
    "classes = [\"Action\", \"Adventure\", \"Animation\", \"Biography\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\",\n",
    "            \"Family\", \"Fantasy\", \"Film-Noir\", \"History\", \"Horror\", \"Music\", \"Musical\", \"Mystery\",\n",
    "            \"Romance\", \"Sci-Fi\", \"Short\", \"Sport\", \"Superhero\", \"Thriller\", \"War\", \"Western\"]\n",
    "\n",
    "for genre in classes:\n",
    "    movie_info[genre] = movie_info[\"genres\"].map(lambda x: genre in x).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4702/4702 [00:02<00:00, 2288.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(movie_info))):\n",
    "    if (imgs_dir / f\"{movie_info.iloc[i].imdb_id}.jpg\").exists():\n",
    "        movie_info[\"imdb_id\"].iloc[i] = str(movie_info.iloc[i][\"imdb_id\"])\n",
    "    elif (imgs_dir / f\"00{movie_info.iloc[i].imdb_id}.jpg\").exists():\n",
    "        movie_info[\"imdb_id\"].iloc[i] = \"00\" + str(movie_info.iloc[i][\"imdb_id\"])\n",
    "    elif (imgs_dir / f\"0{movie_info.iloc[i].imdb_id}.jpg\").exists():\n",
    "        movie_info[\"imdb_id\"].iloc[i] = \"0\" + str(movie_info.iloc[i][\"imdb_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inds = []\n",
    "# for i in tqdm((2487,)):\n",
    "#     try:\n",
    "#         Image.open(imgs_dir / f\"{movie_info.iloc[i].imdb_id}.jpg\").convert(\"RGB\").resize((224, 224))\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         inds.append(i)\n",
    "# len(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving image embeddings for resnet50_features\n",
      "retrieving text embeddings for bag_of_words\n",
      "Using resnet50_features + bag_of_words + <lambda>\n",
      "Embeds size: 27598\n",
      "Score 0.06191386135236041\n",
      "retrieving text embeddings for tf_idf\n",
      "Using resnet50_features + tf_idf + <lambda>\n",
      "Embeds size: 27598\n",
      "Score 0.06213319344385634\n",
      "retrieving text embeddings for spacy_appoach\n",
      "Using resnet50_features + spacy_appoach + <lambda>\n",
      "Embeds size: 2240\n",
      "Score 0.0682355012858485\n",
      "0.0682355012858485 ['resnet50_features', 'spacy_appoach', '<lambda>']\n"
     ]
    }
   ],
   "source": [
    "image_methods = [resnet50_features]\n",
    "text_methods = [bag_of_words, tf_idf, spacy_appoach]\n",
    "classif_methods = [LogisticRegression, lambda: CatBoostClassifier(random_state=42, silent=True, iterations=100)]\n",
    "\n",
    "val_size = 0.2\n",
    "\n",
    "y = movie_info[classes].values\n",
    "y_train, y_val = train_test_split(y, test_size=val_size)\n",
    "\n",
    "best_score = - np.inf\n",
    "for im in image_methods:\n",
    "    print(f\"retrieving image embeddings for {im.__name__}\")\n",
    "    if (embeds_dir / f\"{im.__name__}.npy\").exists():\n",
    "        image_embeds = np.load(embeds_dir / f\"{im.__name__}.npy\")\n",
    "    else:\n",
    "        imgs = [Image.open(imgs_dir / f\"{movie_info.iloc[i].imdb_id}.jpg\").convert(\"RGB\").resize((224, 224)) \n",
    "                            for i in range(len(movie_info))]\n",
    "        image_embeds = []\n",
    "        batch_size = 512\n",
    "        for idx in range(0, len(imgs), batch_size):\n",
    "            image_embeds.append(im(imgs[idx: idx + batch_size]))\n",
    "        image_embeds = np.concatenate(image_embeds, axis=0)\n",
    "        np.save(embeds_dir / f\"{im.__name__}.npy\", image_embeds)\n",
    "\n",
    "    for tm in text_methods:\n",
    "        print(f\"retrieving text embeddings for {tm.__name__}\")\n",
    "        if (embeds_dir / f\"{tm.__name__}_title.npy\").exists():\n",
    "            tm_title_embeds = np.load(embeds_dir / f\"{tm.__name__}_title.npy\")\n",
    "            tm_plot_embeds = np.load(embeds_dir / f\"{tm.__name__}_plot.npy\")\n",
    "        else:\n",
    "            tm_title_embeds, tm_title = tm(movie_info.title.values)\n",
    "            tm_plot_embeds, tm_plot = tm(movie_info[\"plot\"].values)\n",
    "            np.save(embeds_dir / f\"{tm.__name__}_title.npy\", tm_title_embeds)\n",
    "            np.save(embeds_dir / f\"{tm.__name__}_plot.npy\", tm_plot_embeds)\n",
    "\n",
    "        X = np.column_stack([image_embeds, tm_title_embeds, tm_plot_embeds])\n",
    "        X_train, X_val = train_test_split(X, test_size=val_size)\n",
    "        for cm in classif_methods[1:2]:\n",
    "            print(f\"Using {im.__name__} + {tm.__name__} + {cm.__name__}\")\n",
    "            print(f\"Embeds size: {X.shape[1]}\")\n",
    "\n",
    "            models = [cm() for _ in range(len(classes))]\n",
    "            score = 0\n",
    "            for i in range(len(models)):\n",
    "                models[i].fit(X_train, y_train[:, i])\n",
    "                score += f1_score(y_val[:, i], models[i].predict(X_val))\n",
    "            score /= len(models)\n",
    "            print(f\"Score {score}\")\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_comb = (im, tm, cm)\n",
    "print(best_score, [x.__name__ for x in best_comb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method | number of generated features\n",
    "\n",
    "resnet50_features | 2048\n",
    "\n",
    "bag_of_words, tf-idf | 25550\n",
    "\n",
    "spacy | 192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline                                              | mean f1 score\n",
    "\n",
    "resnet50_features + bag_of_words + LogisticRegression | 0.12156873831875155\n",
    "\n",
    "resnet50_features + tf_idf + LogisticRegression       | 0.14007789891666134\n",
    "\n",
    "resnet50_features + spacy_appoach + LogisticRegression| 0.133247543506684\n",
    "\n",
    "resnet50_features + bag_of_words + CatBoostClassifier | 0.06191386135236041\n",
    "\n",
    "resnet50_features + tf_idf + CatBoostClassifier       | 0.06213319344385634\n",
    "\n",
    "resnet50_features + spacy_appoach + CatBoostClassifier| 0.0682355012858485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, classes_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, classes_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = nn.functional.relu(self.fc1(X))\n",
    "        X = nn.functional.relu(self.fc2(X))\n",
    "        X = nn.functional.sigmoid(self.fc3(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving image embeddings for resnet50_features\n",
      "retrieving text embeddings for bag_of_words\n",
      "Epoch #1 loss:  0.39641\n",
      "Epoch #2 loss:  0.16206\n",
      "Epoch #3 loss:  0.14414\n",
      "Epoch #4 loss:  0.14372\n",
      "Epoch #5 loss:  0.14393\n",
      "Score: 0.030349099099099094\n",
      "retrieving text embeddings for tf_idf\n",
      "Epoch #1 loss:  0.41277\n",
      "Epoch #2 loss:  0.17078\n",
      "Epoch #3 loss:  0.14486\n",
      "Epoch #4 loss:  0.14307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000016B6C6AA5E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1424, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000016B6C6AA5E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"c:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1424, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5 loss:  0.14470\n",
      "Score: 0.030349099099099094\n",
      "retrieving text embeddings for spacy_appoach\n",
      "Epoch #1 loss:  0.39228\n",
      "Epoch #2 loss:  0.15688\n",
      "Epoch #3 loss:  0.14460\n",
      "Epoch #4 loss:  0.14381\n",
      "Epoch #5 loss:  0.14393\n",
      "Score: 0.030349099099099094\n"
     ]
    }
   ],
   "source": [
    "image_methods = [resnet50_features]\n",
    "text_methods = [bag_of_words, tf_idf, spacy_appoach]\n",
    "\n",
    "val_size = 0.2\n",
    "\n",
    "y = movie_info[classes].values\n",
    "y_train, y_val = train_test_split(y, test_size=val_size)\n",
    "\n",
    "best_score = - np.inf\n",
    "for im in image_methods:\n",
    "    print(f\"retrieving image embeddings for {im.__name__}\")\n",
    "    if (embeds_dir / f\"{im.__name__}.npy\").exists():\n",
    "        image_embeds = np.load(embeds_dir / f\"{im.__name__}.npy\")\n",
    "    else:\n",
    "        imgs = [Image.open(imgs_dir / f\"{movie_info.iloc[i].imdb_id}.jpg\").convert(\"RGB\").resize((224, 224)) \n",
    "                            for i in range(len(movie_info))]\n",
    "        image_embeds = []\n",
    "        batch_size = 512\n",
    "        for idx in range(0, len(imgs), batch_size):\n",
    "            image_embeds.append(im(imgs[idx: idx + batch_size]))\n",
    "        image_embeds = np.concatenate(image_embeds, axis=0)\n",
    "        np.save(embeds_dir / f\"{im.__name__}.npy\", image_embeds)\n",
    "\n",
    "    for tm in text_methods:\n",
    "        print(f\"retrieving text embeddings for {tm.__name__}\")\n",
    "        if (embeds_dir / f\"{tm.__name__}_title.npy\").exists():\n",
    "            tm_title_embeds = np.load(embeds_dir / f\"{tm.__name__}_title.npy\")\n",
    "            tm_plot_embeds = np.load(embeds_dir / f\"{tm.__name__}_plot.npy\")\n",
    "        else:\n",
    "            tm_title_embeds, tm_title = tm(movie_info.title.values)\n",
    "            tm_plot_embeds, tm_plot = tm(movie_info[\"plot\"].values)\n",
    "            np.save(embeds_dir / f\"{tm.__name__}_title.npy\", tm_title_embeds)\n",
    "            np.save(embeds_dir / f\"{tm.__name__}_plot.npy\", tm_plot_embeds)\n",
    "\n",
    "        X = np.column_stack([image_embeds, tm_title_embeds, tm_plot_embeds])\n",
    "        X_train, X_val = train_test_split(X, test_size=val_size)\n",
    "\n",
    "        # to tensors\n",
    "        batch_size = 512\n",
    "        train_dl = torch.utils.data.DataLoader(list(zip(X_train, y_train)), batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        val_dl = torch.utils.data.DataLoader(list(zip(X_val, y_val)), batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        # training\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = SimpleNet(X.shape[1], len(classes)).float().to(device)\n",
    "        optim = torch.optim.AdamW(model.parameters(), lr=4e-4)\n",
    "        criterion = nn.L1Loss()\n",
    "        epochs = 5\n",
    "        losses = []\n",
    "        model.train()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            losses.append(0)\n",
    "            for X_cur, y_cur in train_dl:\n",
    "                model.zero_grad()\n",
    "                y_hat = model(X_cur.float().to(device))\n",
    "                loss = criterion(y_cur.float().to(device), y_hat)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                losses[- 1] += loss.item()\n",
    "            losses[- 1] /= len(train_dl)\n",
    "            print(f\"Epoch #{epoch} loss: {losses[- 1]: 0.5f}\")\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for X_cur, y_cur in val_dl:\n",
    "                preds.append(model(X_cur.float().to(device)).cpu().numpy())\n",
    "        preds = np.row_stack(preds)\n",
    "        score = np.mean([f1_score(y_val[:, i], (preds[:, i] > 0.5).astype(int)) for i in range(len(classes))])\n",
    "        print(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.56007719039917, 4.925796985626221, 3.1783318028253102, 1.4912840477227125)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_weights = []\n",
    "with torch.no_grad():\n",
    "    for i in range(model.fc1.weight.shape[1]):\n",
    "        avg_weights.append(model.fc1.weight[:, i].sum().item())\n",
    "np.min(avg_weights), np.max(avg_weights), np.mean(avg_weights), np.std(avg_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline with SimpleNet          | mean f1 score\n",
    "\n",
    "resnet50_features + bag_of_words | 0.030349099099099094\n",
    "\n",
    "resnet50_features + tf_idf       | 0.030349099099099094\n",
    "\n",
    "resnet50_features + spacy        | 0.030349099099099094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1289e797c8b2364a1b561fc46768e8fcf8446b2e18e77ab0795c8743ff6ac10a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
