{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from text_features import bag_of_words, tf_idf, spacy_approach\n",
    "from img_features import resnet50_features\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\")\n",
    "imgs_dir = data_dir / \"imgs\"\n",
    "embeds_dir = Path(\"embeddings\")\n",
    "\n",
    "movie_info = pd.read_csv(data_dir / \"movie_info.csv\")\n",
    "movie_info[\"plot\"].fillna(\"No description\", inplace=True)\n",
    "movie_info[\"genres\"] = movie_info[\"genres\"].map(lambda x: json.loads(x.replace(\"\\'\", \"\\\"\")))\n",
    "\n",
    "classes = [\"Action\", \"Adventure\", \"Animation\", \"Biography\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\",\n",
    "            \"Family\", \"Fantasy\", \"Film-Noir\", \"History\", \"Horror\", \"Music\", \"Musical\", \"Mystery\",\n",
    "            \"Romance\", \"Sci-Fi\", \"Short\", \"Sport\", \"Superhero\", \"Thriller\", \"War\", \"Western\"]\n",
    "\n",
    "for genre in classes:\n",
    "    movie_info[genre] = movie_info[\"genres\"].map(lambda x: genre in x).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4702/4702 [00:01<00:00, 2639.74it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(movie_info))):\n",
    "    if (imgs_dir / f\"{movie_info.iloc[i].imdb_id}.jpg\").exists():\n",
    "        movie_info[\"imdb_id\"].iloc[i] = str(movie_info.iloc[i][\"imdb_id\"])\n",
    "    elif (imgs_dir / f\"00{movie_info.iloc[i].imdb_id}.jpg\").exists():\n",
    "        movie_info[\"imdb_id\"].iloc[i] = \"00\" + str(movie_info.iloc[i][\"imdb_id\"])\n",
    "    elif (imgs_dir / f\"0{movie_info.iloc[i].imdb_id}.jpg\").exists():\n",
    "        movie_info[\"imdb_id\"].iloc[i] = \"0\" + str(movie_info.iloc[i][\"imdb_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inds = []\n",
    "# for i in tqdm((2487,)):\n",
    "#     try:\n",
    "#         Image.open(imgs_dir / f\"{movie_info.iloc[i].imdb_id}.jpg\").convert(\"RGB\").resize((224, 224))\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         inds.append(i)\n",
    "# len(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(\"[^ a-zA-Z0-9]\", \" \", text)  # deleting everything besides whitespaces and letters\n",
    "    text = re.sub(\" +\", \" \", text)  # merging multiple whitespaces into one\n",
    "    text = text.lower()  # text to lowercase\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = [word for word in text.split(\" \") if not word in stop_words]  # removing stop_words\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text]  # lemmatization\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in (\"title\", \"plot\"):\n",
    "    movie_info[col] = movie_info[col].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving image embeddings for resnet50_features\n",
      "retrieving text embeddings for spacy_appoach\n",
      "Using resnet50_features + spacy_appoach + LogisticRegression\n",
      "Embeds size: 2240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [03:13<00:00,  8.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.6305036171827988\n",
      "Using resnet50_features + spacy_appoach + CatBoostClassifier\n",
      "Embeds size: 2240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [10:17<00:00, 25.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.5968512397586431\n",
      "0.6305036171827988 ['resnet50_features', 'spacy_appoach', 'LogisticRegression']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_methods = [resnet50_features]\n",
    "text_methods = [bag_of_words, tf_idf, spacy_approach]\n",
    "classif_methods = [LogisticRegression, CatBoostClassifier]\n",
    "classif_methods_params = [{\"random_state\": 0, \"solver\": \"saga\"}, {\"random_state\": 0, \"silent\": True, \"iterations\": 100}]\n",
    "\n",
    "val_size = 0.2\n",
    "\n",
    "y = movie_info[classes].values\n",
    "y_train, y_val = train_test_split(y, test_size=val_size, random_state=0)\n",
    "\n",
    "best_score = - np.inf\n",
    "for im in image_methods:\n",
    "    print(f\"retrieving image embeddings for {im.__name__}\")\n",
    "    if (embeds_dir / f\"{im.__name__}.npy\").exists():\n",
    "        image_embeds = np.load(embeds_dir / f\"{im.__name__}.npy\")\n",
    "    else:\n",
    "        imgs = [Image.open(imgs_dir / f\"{movie_info.iloc[i].imdb_id}.jpg\").convert(\"RGB\").resize((224, 224)) \n",
    "                            for i in range(len(movie_info))]\n",
    "        image_embeds = []\n",
    "        batch_size = 512\n",
    "        for idx in range(0, len(imgs), batch_size):\n",
    "            image_embeds.append(im(imgs[idx: idx + batch_size]))\n",
    "        image_embeds = np.concatenate(image_embeds, axis=0)\n",
    "        np.save(embeds_dir / f\"{im.__name__}.npy\", image_embeds)\n",
    "\n",
    "    for tm in text_methods[2:]:\n",
    "        print(f\"retrieving text embeddings for {tm.__name__}\")\n",
    "        if (embeds_dir / f\"{tm.__name__}_title.npy\").exists():\n",
    "            tm_title_embeds = np.load(embeds_dir / f\"{tm.__name__}_title.npy\")\n",
    "            tm_plot_embeds = np.load(embeds_dir / f\"{tm.__name__}_plot.npy\")\n",
    "        else:\n",
    "            tm_title_embeds, tm_title = tm(movie_info.title.values)\n",
    "            tm_plot_embeds, tm_plot = tm(movie_info[\"plot\"].values)\n",
    "            np.save(embeds_dir / f\"{tm.__name__}_title.npy\", tm_title_embeds)\n",
    "            np.save(embeds_dir / f\"{tm.__name__}_plot.npy\", tm_plot_embeds)\n",
    "\n",
    "        X = np.column_stack([image_embeds, tm_title_embeds, tm_plot_embeds])\n",
    "        X_train, X_val = train_test_split(X, test_size=val_size, random_state=0)\n",
    "        for cm, cmp in zip(classif_methods, classif_methods_params):\n",
    "            print(f\"Using {im.__name__} + {tm.__name__} + {cm.__name__}\")\n",
    "            print(f\"Embeds size: {X.shape[1]}\")\n",
    "\n",
    "            models = [cm(**cmp) for _ in range(len(classes))]\n",
    "            score = 0\n",
    "            for i in tqdm(range(len(models))):\n",
    "                models[i].fit(X_train, y_train[:, i])\n",
    "                score += f1_score(y_val[:, i], models[i].predict(X_val))\n",
    "            score /= len(models)\n",
    "            print(f\"Score {score}\")\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_comb = (im, tm, cm)\n",
    "print(best_score, [x.__name__ for x in best_comb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method | number of generated features\n",
    "\n",
    "resnet50_features | 2048\n",
    "\n",
    "bag_of_words, tf-idf | 22997\n",
    "\n",
    "spacy | 192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline                                               | mean f1 score\n",
    "\n",
    "resnet50_features + bag_of_words + LogisticRegression  | 0.67489\n",
    "\n",
    "resnet50_features + tf_idf + LogisticRegression        | 0.63171\n",
    "\n",
    "resnet50_features + spacy_approach + LogisticRegression | 0.63050\n",
    "\n",
    "resnet50_features + bag_of_words + CatBoostClassifier  | 0.57421\n",
    "\n",
    "resnet50_features + tf_idf + CatBoostClassifier        | 0.57266\n",
    "\n",
    "resnet50_features + spacy_approach + CatBoostClassifier | 0.59685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, classes_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, classes_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = nn.functional.relu(self.fc1(X))\n",
    "        X = nn.functional.relu(self.fc2(X))\n",
    "        X = nn.functional.sigmoid(self.fc3(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving image embeddings for resnet50_features\n",
      "retrieving text embeddings for bag_of_words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 loss:  0.21696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2 loss:  0.14580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3 loss:  0.14559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4 loss:  0.14557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5 loss:  0.14485\n",
      "Score: 0.03177733946964716\n",
      "retrieving text embeddings for tf_idf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 loss:  0.22168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2 loss:  0.14566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3 loss:  0.14548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4 loss:  0.14552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5 loss:  0.14576\n",
      "Score: 0.03177733946964716\n",
      "retrieving text embeddings for spacy_appoach\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 loss:  0.21506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2 loss:  0.14554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3 loss:  0.14568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4 loss:  0.14524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5 loss:  0.14566\n",
      "Score: 0.03177733946964716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "image_methods = [resnet50_features]\n",
    "text_methods = [bag_of_words, tf_idf, spacy_approach]\n",
    "\n",
    "val_size = 0.2\n",
    "\n",
    "y = movie_info[classes].values\n",
    "\n",
    "best_score = - np.inf\n",
    "for im in image_methods:\n",
    "    print(f\"retrieving image embeddings for {im.__name__}\")\n",
    "    if (embeds_dir / f\"{im.__name__}.npy\").exists():\n",
    "        image_embeds = np.load(embeds_dir / f\"{im.__name__}.npy\")\n",
    "    else:\n",
    "        imgs = [Image.open(imgs_dir / f\"{movie_info.iloc[i].imdb_id}.jpg\").convert(\"RGB\").resize((224, 224)) \n",
    "                            for i in range(len(movie_info))]\n",
    "        image_embeds = []\n",
    "        batch_size = 512\n",
    "        for idx in range(0, len(imgs), batch_size):\n",
    "            image_embeds.append(im(imgs[idx: idx + batch_size]))\n",
    "        image_embeds = np.concatenate(image_embeds, axis=0)\n",
    "        np.save(embeds_dir / f\"{im.__name__}.npy\", image_embeds)\n",
    "\n",
    "    for tm in text_methods:\n",
    "        print(f\"retrieving text embeddings for {tm.__name__}\")\n",
    "        if (embeds_dir / f\"{tm.__name__}_title.npy\").exists():\n",
    "            tm_title_embeds = np.load(embeds_dir / f\"{tm.__name__}_title.npy\")\n",
    "            tm_plot_embeds = np.load(embeds_dir / f\"{tm.__name__}_plot.npy\")\n",
    "        else:\n",
    "            tm_title_embeds, tm_title = tm(movie_info.title.values)\n",
    "            tm_plot_embeds, tm_plot = tm(movie_info[\"plot\"].values)\n",
    "            np.save(embeds_dir / f\"{tm.__name__}_title.npy\", tm_title_embeds)\n",
    "            np.save(embeds_dir / f\"{tm.__name__}_plot.npy\", tm_plot_embeds)\n",
    "\n",
    "        X = np.column_stack([image_embeds, tm_title_embeds, tm_plot_embeds])\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=0)\n",
    "\n",
    "        # to tensors\n",
    "        batch_size = 128\n",
    "        train_dl = torch.utils.data.DataLoader(list(zip(X_train, y_train)), batch_size=batch_size, shuffle=True)\n",
    "        val_dl = torch.utils.data.DataLoader(list(zip(X_val, y_val)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # training\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = SimpleNet(X.shape[1], len(classes)).float().to(device)\n",
    "        optim = torch.optim.AdamW(model.parameters(), lr=4e-4)\n",
    "        criterion = nn.L1Loss()\n",
    "        epochs = 5\n",
    "        losses = []\n",
    "        model.train()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            losses.append(0)\n",
    "            for X_cur, y_cur in tqdm(train_dl, leave=False):\n",
    "                model.zero_grad()\n",
    "                y_hat = model(X_cur.float().to(device))\n",
    "                loss = criterion(y_cur.float().to(device), y_hat)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                losses[- 1] += loss.item()\n",
    "            losses[- 1] /= len(train_dl)\n",
    "            print(f\"Epoch #{epoch} loss: {losses[- 1]: 0.5f}\")\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for X_cur, y_cur in val_dl:\n",
    "                preds.append(model(X_cur.float().to(device)).cpu().numpy())\n",
    "        preds = np.row_stack(preds)\n",
    "        score = np.mean([f1_score(y_val[:, i], (preds[:, i] > 0.5).astype(int)) for i in range(len(classes))])\n",
    "        print(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.56007719039917, 4.925796985626221, 3.1783318028253102, 1.4912840477227125)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_weights = []\n",
    "with torch.no_grad():\n",
    "    for i in range(model.fc1.weight.shape[1]):\n",
    "        avg_weights.append(model.fc1.weight[:, i].sum().item())\n",
    "np.min(avg_weights), np.max(avg_weights), np.mean(avg_weights), np.std(avg_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline with SimpleNet          | mean f1 score\n",
    "\n",
    "resnet50_features + bag_of_words | 0.03177\n",
    "\n",
    "resnet50_features + tf_idf       | 0.03177\n",
    "\n",
    "resnet50_features + spacy        | 0.03177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1289e797c8b2364a1b561fc46768e8fcf8446b2e18e77ab0795c8743ff6ac10a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
