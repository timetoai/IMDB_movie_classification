{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "!pip install transformers\n",
        "! pip install sentence_transformers\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "A_wNGKEFmA31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a3264f-fc1b-4184-ec07-8781f5133d97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.7.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2022.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.24.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.97)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.10.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EjMsJYpClrhK"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import re\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from text_features_3 import bag_of_words, tf_idf, spacy_approach, word2vec_approach, transformer_clip, transformer_distil_bert\n",
        "from img_features import resnet50_features\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from catboost import CatBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# raif_trans = pd.read_csv('/content/drive/My Drive/Raifdata.csv', header=0, sep=',')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbgMa4BvOB23",
        "outputId": "7bf4b317-c31e-4071-8e4f-7b7e08c50697"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-5AsfxzqlrhM"
      },
      "outputs": [],
      "source": [
        "data_dir = Path(\"/content/drive/My Drive\")\n",
        "imgs_dir = data_dir / \"imgs\"\n",
        "embeds_dir = Path(\"embeddings\")\n",
        "\n",
        "movie_info = pd.read_csv(data_dir / \"movie_info.csv\")\n",
        "movie_info[\"plot\"].fillna(\"No description\", inplace=True)\n",
        "movie_info[\"genres\"] = movie_info[\"genres\"].map(lambda x: json.loads(x.replace(\"\\'\", \"\\\"\")))\n",
        "\n",
        "classes = [\"Action\", \"Adventure\", \"Animation\", \"Biography\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\",\n",
        "            \"Family\", \"Fantasy\", \"Film-Noir\", \"History\", \"Horror\", \"Music\", \"Musical\", \"Mystery\",\n",
        "            \"Romance\", \"Sci-Fi\", \"Short\", \"Sport\", \"Superhero\", \"Thriller\", \"War\", \"Western\"]\n",
        "\n",
        "for genre in classes:\n",
        "    movie_info[genre] = movie_info[\"genres\"].map(lambda x: genre in x).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vSdrMMu8lrhN",
        "outputId": "dbbd40b4-7ae6-4198-de7f-42debc847013",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4702/4702 [00:07<00:00, 640.74it/s]\n"
          ]
        }
      ],
      "source": [
        "for i in tqdm(range(len(movie_info))):\n",
        "    if (imgs_dir / f\"{movie_info.iloc[i].imdb_id}.jpg\").exists():\n",
        "        movie_info[\"imdb_id\"].iloc[i] = str(movie_info.iloc[i][\"imdb_id\"])\n",
        "    elif (imgs_dir / f\"00{movie_info.iloc[i].imdb_id}.jpg\").exists():\n",
        "        movie_info[\"imdb_id\"].iloc[i] = \"00\" + str(movie_info.iloc[i][\"imdb_id\"])\n",
        "    elif (imgs_dir / f\"0{movie_info.iloc[i].imdb_id}.jpg\").exists():\n",
        "        movie_info[\"imdb_id\"].iloc[i] = \"0\" + str(movie_info.iloc[i][\"imdb_id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3PtfaFY8lrhP"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(\"[^ a-zA-Z0-9]\", \" \", text)  # deleting everything besides whitespaces and letters\n",
        "    text = re.sub(\" +\", \" \", text)  # merging multiple whitespaces into one\n",
        "    text = text.lower()  # text to lowercase\n",
        "\n",
        "    stop_words = stopwords.words('english')\n",
        "    text = [word for word in text.split(\" \") if not word in stop_words]  # removing stop_words\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(token) for token in text]  # lemmatization\n",
        "    return ' '.join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ciK65OuXlrhP"
      },
      "outputs": [],
      "source": [
        "for col in (\"title\", \"plot\"):\n",
        "    movie_info[col] = movie_info[col].map(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9pRLAvFfCwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HHk85Y6nlrhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b386c44a-3b79-47e3-aae5-cfd1dc928751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retrieving image embeddings for transformer_clip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retrieving text embeddings for transformer_clip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n",
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using transformer_clip + transformer_clip + LogisticRegression\n",
            "Embeds size: 1536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [03:08<00:00,  7.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score 0.722937328444511\n",
            "retrieving text embeddings for transformer_distil_bert\n",
            "Using transformer_clip + transformer_distil_bert + LogisticRegression\n",
            "Embeds size: 2048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [04:12<00:00, 10.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score 0.6607348537810084\n",
            "retrieving image embeddings for resnet50_features\n",
            "retrieving text embeddings for transformer_clip\n",
            "Using resnet50_features + transformer_clip + LogisticRegression\n",
            "Embeds size: 3072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [06:13<00:00, 15.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score 0.7036974529671972\n",
            "retrieving text embeddings for transformer_distil_bert\n",
            "Using resnet50_features + transformer_distil_bert + LogisticRegression\n",
            "Embeds size: 3584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [07:17<00:00, 18.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score 0.64425909164199\n",
            "0.722937328444511 ['transformer_clip', 'transformer_clip', 'LogisticRegression']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "image_methods = [transformer_clip, resnet50_features]\n",
        "text_methods = [transformer_clip, transformer_distil_bert]#, bag_of_words, tf_idf, spacy_approach]\n",
        "classif_methods = [LogisticRegression]#, CatBoostClassifier]\n",
        "classif_methods_params = [{\"random_state\": 0, \"solver\": \"saga\"}, {\"random_state\": 0, \"silent\": True, \"iterations\": 100}]\n",
        "\n",
        "val_size = 0.2\n",
        "\n",
        "y = movie_info[classes].values\n",
        "y_train, y_val = train_test_split(y, test_size=val_size, random_state=0)\n",
        "\n",
        "best_score = - np.inf\n",
        "for im in image_methods:\n",
        "    print(f\"retrieving image embeddings for {im.__name__}\")\n",
        "    if (embeds_dir / f\"{im.__name__}.npy\").exists():\n",
        "        image_embeds = np.load(embeds_dir / f\"{im.__name__}.npy\")\n",
        "    else:\n",
        "        imgs = [Image.open(imgs_dir / f\"{movie_info.iloc[i].imdb_id}.jpg\").convert(\"RGB\").resize((224, 224)) \n",
        "                            for i in range(len(movie_info))]\n",
        "        \n",
        "        image_embeds, im_model = im(imgs)\n",
        "        image_embeds = np.array(image_embeds)\n",
        "        \n",
        "        np.save(embeds_dir / f\"{im.__name__}.npy\", image_embeds)\n",
        "\n",
        "    for tm in text_methods:\n",
        "        print(f\"retrieving text embeddings for {tm.__name__}\")\n",
        "        if (embeds_dir / f\"{tm.__name__}_title.npy\").exists():\n",
        "            tm_title_embeds = np.load(embeds_dir / f\"{tm.__name__}_title.npy\")\n",
        "            tm_plot_embeds = np.load(embeds_dir / f\"{tm.__name__}_plot.npy\")\n",
        "        else:\n",
        "            tm_title_embeds, tm_title = tm(movie_info.title.values, data_type_text=True)\n",
        "            tm_plot_embeds, tm_plot = tm(movie_info[\"plot\"].values, data_type_text=True)\n",
        "\n",
        "            np.save(embeds_dir / f\"{tm.__name__}_title.npy\", tm_title_embeds)\n",
        "            np.save(embeds_dir / f\"{tm.__name__}_plot.npy\", tm_plot_embeds)\n",
        "\n",
        "        X = np.column_stack([image_embeds, tm_title_embeds, tm_plot_embeds])\n",
        "        X_train, X_val = train_test_split(X, test_size=val_size, random_state=0)\n",
        "\n",
        "        for cm, cmp in zip(classif_methods, classif_methods_params):\n",
        "            print(f\"Using {im.__name__} + {tm.__name__} + {cm.__name__}\")\n",
        "            print(f\"Embeds size: {X.shape[1]}\")\n",
        "\n",
        "            models = [cm(**cmp) for _ in range(len(classes))]\n",
        "            score = 0\n",
        "            for i in tqdm(range(len(models))):\n",
        "\n",
        "                models[i].fit(X_train, y_train[:, i])\n",
        "                score += f1_score(y_val[:, i], models[i].predict(X_val))\n",
        "            score /= len(models)\n",
        "            print(f\"Score {score}\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_comb = (im, tm, cm)\n",
        "print(best_score, [x.__name__ for x in best_comb])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "weL8xlbRRTvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpnXrujh1CFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBqNx05KlrhR"
      },
      "source": [
        "Method | number of generated features\n",
        "\n",
        "resnet50_features | 2048\n",
        "\n",
        "bag_of_words, tf-idf | 22997\n",
        "\n",
        "spacy | 192\n",
        "\n",
        "word2vec | 22798"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwHTfgbslrhS"
      },
      "source": [
        "Pipeline                                               | mean f1 score\n",
        "\n",
        "resnet50_features + bag_of_words + LogisticRegression  | 0.67489\n",
        "\n",
        "resnet50_features + tf_idf + LogisticRegression        | 0.63171\n",
        "\n",
        "resnet50_features + spacy_approach + LogisticRegression | 0.63050\n",
        "\n",
        "resnet50_features + word2vec_approach + LogisticRegression | 0.5764033562527832\n",
        "\n",
        "\n",
        "resnet50_features + bag_of_words + CatBoostClassifier  | 0.57421\n",
        "\n",
        "resnet50_features + tf_idf + CatBoostClassifier        | 0.57266\n",
        "\n",
        "resnet50_features + spacy_approach + CatBoostClassifier | 0.59685\n",
        "\n",
        "resnet50_features + word2vec_approach + CatBoostClassifier | 0.5720593239340869\n",
        "\n",
        "-------------------------------------------------\n",
        "#### with transformers \\\\/\n",
        "-------------------------------------------------\n",
        "\n",
        "transformer_clip + transformer_clip + LogisticRegression | 0.722937328444511\n",
        "\n",
        "transformer_clip + transformer_distil_bert + LogisticRegression | 0.6607348537810084\n",
        "\n",
        "resnet50_features + transformer_clip + LogisticRegression | 0.7036974529671972\n",
        "\n",
        "resnet50_features + transformer_distil_bert + LogisticRegression | 0.64425909164199"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LPBXtedlrhT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDPD4zT_lrhT"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, input_size, classes_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, classes_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = nn.functional.relu(self.fc1(X))\n",
        "        X = nn.functional.relu(self.fc2(X))\n",
        "        X = nn.functional.sigmoid(self.fc3(X))\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS4yz0Z2lrhT"
      },
      "outputs": [],
      "source": [
        "image_methods = [resnet50_features]\n",
        "text_methods = [bag_of_words, tf_idf, spacy_approach]\n",
        "\n",
        "val_size = 0.2\n",
        "\n",
        "y = movie_info[classes].values\n",
        "\n",
        "best_score = - np.inf\n",
        "for im in image_methods:\n",
        "    print(f\"retrieving image embeddings for {im.__name__}\")\n",
        "    if (embeds_dir / f\"{im.__name__}.npy\").exists():\n",
        "        image_embeds = np.load(embeds_dir / f\"{im.__name__}.npy\")\n",
        "    else:\n",
        "        imgs = [Image.open(imgs_dir / f\"{movie_info.iloc[i].imdb_id}.jpg\").convert(\"RGB\").resize((224, 224)) \n",
        "                            for i in range(len(movie_info))]\n",
        "        image_embeds = []\n",
        "        batch_size = 512\n",
        "        for idx in range(0, len(imgs), batch_size):\n",
        "            image_embeds.append(im(imgs[idx: idx + batch_size]))\n",
        "        image_embeds = np.concatenate(image_embeds, axis=0)\n",
        "        np.save(embeds_dir / f\"{im.__name__}.npy\", image_embeds)\n",
        "\n",
        "    for tm in text_methods:\n",
        "        print(f\"retrieving text embeddings for {tm.__name__}\")\n",
        "        if (embeds_dir / f\"{tm.__name__}_title.npy\").exists():\n",
        "            tm_title_embeds = np.load(embeds_dir / f\"{tm.__name__}_title.npy\")\n",
        "            tm_plot_embeds = np.load(embeds_dir / f\"{tm.__name__}_plot.npy\")\n",
        "        else:\n",
        "            tm_title_embeds, tm_title = tm(movie_info.title.values)\n",
        "            tm_plot_embeds, tm_plot = tm(movie_info[\"plot\"].values)\n",
        "            np.save(embeds_dir / f\"{tm.__name__}_title.npy\", tm_title_embeds)\n",
        "            np.save(embeds_dir / f\"{tm.__name__}_plot.npy\", tm_plot_embeds)\n",
        "\n",
        "        X = np.column_stack([image_embeds, tm_title_embeds, tm_plot_embeds])\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=0)\n",
        "\n",
        "        # to tensors\n",
        "        batch_size = 128\n",
        "        train_dl = torch.utils.data.DataLoader(list(zip(X_train, y_train)), batch_size=batch_size, shuffle=True)\n",
        "        val_dl = torch.utils.data.DataLoader(list(zip(X_val, y_val)), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # training\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = SimpleNet(X.shape[1], len(classes)).float().to(device)\n",
        "        optim = torch.optim.AdamW(model.parameters(), lr=4e-4)\n",
        "        criterion = nn.L1Loss()\n",
        "        epochs = 5\n",
        "        losses = []\n",
        "        model.train()\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            losses.append(0)\n",
        "            for X_cur, y_cur in tqdm(train_dl, leave=False):\n",
        "                model.zero_grad()\n",
        "                y_hat = model(X_cur.float().to(device))\n",
        "                loss = criterion(y_cur.float().to(device), y_hat)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                losses[- 1] += loss.item()\n",
        "            losses[- 1] /= len(train_dl)\n",
        "            print(f\"Epoch #{epoch} loss: {losses[- 1]: 0.5f}\")\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for X_cur, y_cur in val_dl:\n",
        "                preds.append(model(X_cur.float().to(device)).cpu().numpy())\n",
        "        preds = np.row_stack(preds)\n",
        "        score = np.mean([f1_score(y_val[:, i], (preds[:, i] > 0.5).astype(int)) for i in range(len(classes))])\n",
        "        print(f\"Score: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hHk21KwlrhU"
      },
      "outputs": [],
      "source": [
        "avg_weights = []\n",
        "with torch.no_grad():\n",
        "    for i in range(model.fc1.weight.shape[1]):\n",
        "        avg_weights.append(model.fc1.weight[:, i].sum().item())\n",
        "np.min(avg_weights), np.max(avg_weights), np.mean(avg_weights), np.std(avg_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVR5frFvlrhV"
      },
      "source": [
        "Pipeline with SimpleNet          | mean f1 score\n",
        "\n",
        "resnet50_features + bag_of_words | 0.03177\n",
        "\n",
        "resnet50_features + tf_idf       | 0.03177\n",
        "\n",
        "resnet50_features + spacy        | 0.03177"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4sgqdv-lrhV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1289e797c8b2364a1b561fc46768e8fcf8446b2e18e77ab0795c8743ff6ac10a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}